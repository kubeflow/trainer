apiVersion: trainer.kubeflow.org/v1alpha1
kind: ClusterTrainingRuntime
metadata:
  name: torchtune-llama3.2-3b
spec:
  mlPolicy:
    numNodes: 1
    torch:
      numProcPerNode: 2
  template:
    spec:
      replicatedJobs:
        # TODO(Electronic-Waste): Add dataset initializer when we support loading custom datasets.
        - name: model-initializer
          template:
            metadata:
              labels:
                trainer.kubeflow.org/trainjob-ancestor-step: model-initializer
            spec:
              template:
                spec:
                  containers:
                    - name: model-initializer
                      image: ghcr.io/kubeflow/trainer/model-initializer
                      env:
                        - name: STORAGE_URI
                          value: hf://meta-llama/Llama-3.2-3B-Instruct
                      volumeMounts:
                        - name: model-initializer
                          mountPath: /workspace/model
                  volumes:
                    - name: model-initializer
                      persistentVolumeClaim:
                        claimName: model-initializer
        - name: node
          template:
            metadata:
              labels:
                trainer.kubeflow.org/trainjob-ancestor-step: trainer
            spec:
              template:
                spec:
                  containers:
                    - name: node
                      image: ghcr.io/kubeflow/trainer/torchtune-trainer
                      command:
                         - /bin/bash
                          - -c
                          - |
                            echo "TorchTune Llama3.2-3B Finetuning Runtime"

                            echo "--------------------------------------"
                            pip list
