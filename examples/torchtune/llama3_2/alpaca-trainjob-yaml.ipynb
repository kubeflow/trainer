{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c49a6d5",
   "metadata": {},
   "source": [
    "# Fine-tune Llama-3.2-1B-Instruct with Alpaca Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b15eff",
   "metadata": {},
   "source": [
    "This example demonstrates how to fine-tune Llama-3.2-1B-Instruct model with the Alpaca Dataset using TorchTune `BuiltinTrainer` from Kubeflow Trainer SDK.\n",
    "\n",
    "This notebooks walks you through the prerequisites of using TorchTune `BuiltinTrainer` from Kubeflow Trainer SDK, and how to submit TrainJob to bootstrap the fine-tuning workflow.\n",
    "\n",
    "Llama-3.2-1B-Instruct: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "\n",
    "Alpaca Dataset: https://huggingface.co/datasets/tatsu-lab/alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a60eb",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ec515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kubeflow/sdk.git@main#subdirectory=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211fbf9",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Install Official Training Runtimes\n",
    "\n",
    "You need to make sure that you've installed the Kubeflow Trainer Controller Manager and Kubeflow Training Runtimes mentioned in the [installation guide](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35e4fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='deepspeed-distributed', trainer=Trainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework=<Framework.DEEPSPEED: 'deepspeed'>, entrypoint=['mpirun', '--hostfile', '/etc/mpi/hostfile', 'bash', '-c'], accelerator='Unknown', accelerator_count=4), pretrained_model=None)\n",
      "Runtime(name='mlx-distributed', trainer=Trainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework=<Framework.MLX: 'mlx'>, entrypoint=['mpirun', '--hostfile', '/etc/mpi/hostfile', 'bash', '-c'], accelerator='Unknown', accelerator_count=1), pretrained_model=None)\n",
      "Runtime(name='mpi-distributed', trainer=Trainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework=<Framework.TORCH: 'torch'>, entrypoint=['torchrun'], accelerator='Unknown', accelerator_count=1), pretrained_model=None)\n",
      "Runtime(name='torch-distributed', trainer=Trainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework=<Framework.TORCH: 'torch'>, entrypoint=['torchrun'], accelerator='Unknown', accelerator_count='Unknown'), pretrained_model=None)\n",
      "Runtime(name='torchtune-llama3.2-1b', trainer=Trainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework=<Framework.TORCHTUNE: 'torchtune'>, entrypoint=['tune', 'run'], accelerator='Unknown', accelerator_count=2), pretrained_model=None)\n",
      "Runtime(name='torchtune-llama3.2-3b', trainer=Trainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework=<Framework.TORCHTUNE: 'torchtune'>, entrypoint=['tune', 'run'], accelerator='Unknown', accelerator_count=2), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "# List all available Kubeflow Training Runtimes.\n",
    "from kubeflow.trainer import TrainerClient\n",
    "\n",
    "client = TrainerClient(namespace=\"kubeflow\")\n",
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5f256",
   "metadata": {},
   "source": [
    "### Create PVCs for Models and Datasets\n",
    "\n",
    "Currently, we do not support automatically orchestrate the volume claim in (Cluster)TrainingRuntime.\n",
    "\n",
    "So, we need to manually create PVCs for each models we want to fine-tune. In this example, we'll create a PVC `torchtune-llama3.2-1b`, of which name is the same with the ClusterTrainingRuntime we want to use.\n",
    "\n",
    "REF: https://github.com/kubeflow/trainer/issues/2630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11cc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/torchtune-llama3.2-1b created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: torchtune-llama3.2-1b\n",
    "  namespace: kubeflow\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 20Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490f66",
   "metadata": {},
   "source": [
    "## Bootstrap LLM Fine-tuning Workflow\n",
    "\n",
    "Kubeflow TrainJob will train the model in the referenced (Cluster)TrainingRuntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainjob.trainer.kubeflow.org/torchtune-llama3-2-1b created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HF_TOKEN=\"<YOUR_HF_TOKEN_HERE>\"\n",
    "\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: trainer.kubeflow.org/v1alpha1\n",
    "kind: TrainJob\n",
    "metadata:\n",
    "  name: torchtune-llama3-2-1b\n",
    "  namespace: kubeflow\n",
    "spec:\n",
    "  runtimeRef:\n",
    "    name: torchtune-llama3.2-1b\n",
    "  trainer:\n",
    "    resourcesPerNode:\n",
    "      requests:\n",
    "        nvidia.com/gpu: 1\n",
    "      limits:\n",
    "        nvidia.com/gpu: 1\n",
    "    numProcPerNode: 1\n",
    "  initializer:\n",
    "    model:\n",
    "      env:\n",
    "        - name: ACCESS_TOKEN\n",
    "          value: \"${HF_TOKEN}\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a82b76",
   "metadata": {},
   "source": [
    "## Watch the TrainJob logs\n",
    "\n",
    "We can use the `get_job_logs()` API to get the TrainJob logs.\n",
    "\n",
    "### Dataset Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e9d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15T07:29:54Z INFO     [__main__.py:16] Starting dataset initialization\n",
      "2025-06-15T07:29:54Z INFO     [huggingface.py:28] Downloading dataset: tatsu-lab/alpaca\n",
      "2025-06-15T07:29:54Z INFO     [huggingface.py:29] ----------------------------------------\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 926.78it/s]\n",
      "2025-06-15T07:29:55Z INFO     [huggingface.py:40] Dataset has been downloaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer.constants import constants\n",
    "\n",
    "log_dict = client.get_job_logs(\"torchtune-llama3-2-1b\", follow=False, step=constants.DATASET_INITIALIZER)\n",
    "print(log_dict[constants.DATASET_INITIALIZER])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f970c03",
   "metadata": {},
   "source": [
    "### Model Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ce8c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15T07:30:06Z INFO     [__main__.py:16] Starting pre-trained model initialization\n",
      "2025-06-15T07:30:06Z INFO     [huggingface.py:26] Downloading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "2025-06-15T07:30:06Z INFO     [huggingface.py:27] ----------------------------------------\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:33<00:00,  4.25s/it]\n",
      "2025-06-15T07:30:41Z INFO     [huggingface.py:43] Model has been downloaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_dict = client.get_job_logs(\"torchtune-llama3-2-1b\", follow=False, step=constants.MODEL_INITIALIZER)\n",
    "print(log_dict[constants.MODEL_INITIALIZER])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67775ea",
   "metadata": {},
   "source": [
    "### Trainer Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae672d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /workspace/model\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: LLAMA3_2\n",
      "  output_dir: /workspace/output/model\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.instruct_dataset\n",
      "  data_dir: /workspace/dataset/data\n",
      "  packed: false\n",
      "  source: parquet\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: false\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /workspace/output/model/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_2.llama3_2_1b\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "optimizer_in_bwd: false\n",
      "output_dir: /workspace/output/model\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /workspace/output/model/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /workspace/model/original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 620223268. Local seed is seed + rank = 620223268 + 0\n",
      "Writing logs to /workspace/output/model/logs/log_1749972656.txt\n",
      "INFO:torchtune.utils._logging:Distributed training is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 11.91 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.33 GiB\n",
      "\tGPU peak memory reserved: 2.34 GiB\n",
      "\tGPU peak memory active: 2.33 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "Generating train split: 52002 examples [00:00, 181488.38 examples/s]\n",
      "INFO:torchtune.utils._logging:No learning rate scheduler configured. Using constant learning rate.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|1333|Loss: 1.6774815320968628:  82%|████████▏ | 1334/1625 [20:26<04:33,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "log_dict = client.get_job_logs(\"torchtune-llama3-2-1b\", follow=False)\n",
    "print(log_dict[f\"{constants.NODE}-0\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training-operator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
