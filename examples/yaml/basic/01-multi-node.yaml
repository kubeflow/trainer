# Example 1: Multi-Node Distributed Training
#
# This example demonstrates multi-node distributed training using the
# torch-distributed ClusterTrainingRuntime. It uses the default PyTorch
# image provided by the runtime.
#
# The trainer controller sets PET_* environment variables which torchrun
# uses to configure distributed training.
#
# Apply: kubectl apply -f 01-multi-node.yaml
# Check: kubectl get trainjobs multi-node-example
# Logs:  kubectl logs -l trainer.kubeflow.org/job-name=multi-node-example
# Clean: kubectl delete trainjob multi-node-example

apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: multi-node-example
  namespace: default
spec:
  runtimeRef:
    apiGroup: trainer.kubeflow.org
    kind: ClusterTrainingRuntime
    name: torch-distributed

  trainer:
    # Run on 3 nodes to simulate distributed training
    numNodes: 3

    # No image specified - uses the default from torch-distributed runtime
    # (pytorch/pytorch:2.9.1-cuda12.8-cudnn9-runtime)

    # Command that prints distributed training environment info
    command:
      - python3
      - -c
      - |
        import os
        import time

        print("=========================================")
        print("Multi-Node Distributed Training")
        print("=========================================")

        # PET_* env vars are set by the Kubeflow Trainer controller
        # and consumed by torchrun to configure distributed training
        print(f"PET_NNODES: {os.getenv('PET_NNODES', 'not set')}")
        print(f"PET_NPROC_PER_NODE: {os.getenv('PET_NPROC_PER_NODE', 'not set')}")
        print(f"PET_NODE_RANK: {os.getenv('PET_NODE_RANK', 'not set')}")
        print(f"PET_MASTER_ADDR: {os.getenv('PET_MASTER_ADDR', 'not set')}")
        print(f"PET_MASTER_PORT: {os.getenv('PET_MASTER_PORT', 'not set')}")
        print("=========================================")

        node_rank = os.getenv('PET_NODE_RANK', '0')

        # Simulate some work
        print("Simulating training...")
        for i in range(1, 6):
            print(f"Epoch {i}: Processing batch on node rank {node_rank}")
            time.sleep(2)

        print("=========================================")
        print(f"Training completed on node rank {node_rank}")
        print("=========================================")
