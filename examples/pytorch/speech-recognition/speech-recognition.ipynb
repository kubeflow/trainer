{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PyTorch Speech Recognition with Kubeflow Trainer\n",
                "\n",
                "This example demonstrates how to train a speech recognition model using the [Google Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) dataset and PyTorch Distributed Data Parallel (DDP).\n",
                "\n",
                "It follows a development workflow designed for scale:\n",
                "1. **Define**: Wrap training logic in a self-contained function.\n",
                "2. **Test Locally**: Run the function in a local subprocess to verify code correctness.\n",
                "3. **Scale Distributed**: Submit the same function as a distributed `TrainJob` to a Kubernetes cluster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation\n",
                "\n",
                "Install the Kubeflow Trainer SDK and the model dependencies (PyTorch, Torchaudio, etc)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Kubeflow Trainer SDK\n",
                "# Use the package from PyPI for standard usage\n",
                "!pip install -q kubeflow\n",
                "\n",
                "# For SDK Development only:\n",
                "# !pip install -q -e ../../../api/python_api\n",
                "\n",
                "# Install model dependencies\n",
                "!pip install -q torch torchaudio librosa soundfile tensorboard"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Training Function\n",
                "\n",
                "We define `train_speech_recognition` to encapsulate the entire training loop.\n",
                "\n",
                "**Critical Requirement**: This function must be self-contained. It must import all necessary libraries inside itself because it will be serialized and executed in an isolated environment (container or subprocess)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_speech_recognition():\n",
                "    \"\"\"\n",
                "    Trains an Audio Transformer on the Speech Commands dataset using PyTorch DDP.\n",
                "    \"\"\"\n",
                "    import os\n",
                "    import torch\n",
                "    import torch.nn as nn\n",
                "    import torch.distributed as dist\n",
                "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
                "    from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
                "    import torchaudio\n",
                "    import random\n",
                "    from pathlib import Path\n",
                "    import platform\n",
                "    from datetime import timedelta\n",
                "    import warnings\n",
                "\n",
                "    # Suppress warnings and progress bars for cleaner output\n",
                "    warnings.filterwarnings('ignore')\n",
                "    os.environ['TORCHAUDIO_DOWNLOAD_PROGRESS'] = '0'\n",
                "\n",
                "    # --- 1. Environment & Configuration ---\n",
                "    # Detect if we are running on Windows (Local) or Linux (Cluster/Container)\n",
                "    if platform.system() == 'Windows':\n",
                "        data_path = Path(\"C:/data\")\n",
                "        output_dir = Path(\"C:/output\")\n",
                "    else:\n",
                "        data_path = Path.home() / \"data\"\n",
                "        output_dir = Path.home() / \"output\"\n",
                "    \n",
                "    data_path.mkdir(parents=True, exist_ok=True)\n",
                "    output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    # Hyperparameters\n",
                "    BATCH_SIZE = 32\n",
                "    EPOCHS = 3\n",
                "    LR = 0.001\n",
                "\n",
                "    # --- 2. DDP Initialization ---\n",
                "    def setup_ddp():\n",
                "        \"\"\"Initializes the distributed process group.\"\"\"\n",
                "        if all(k in os.environ for k in [\"LOCAL_RANK\", \"RANK\", \"WORLD_SIZE\"]):\n",
                "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
                "            rank = int(os.environ[\"RANK\"])\n",
                "            world_size = int(os.environ[\"WORLD_SIZE\"])\n",
                "            \n",
                "            if torch.cuda.is_available():\n",
                "                backend = \"nccl\"\n",
                "                device = torch.device(\"cuda\", local_rank)\n",
                "                torch.cuda.set_device(device)\n",
                "            else:\n",
                "                backend = \"gloo\"\n",
                "                device = torch.device(\"cpu\")\n",
                "            \n",
                "            dist.init_process_group(backend=backend, timeout=timedelta(minutes=60))\n",
                "            \n",
                "            print(f\"[Rank {rank}/{world_size}] DDP Initialized using {backend}\")\n",
                "            return device, local_rank, rank, world_size\n",
                "        else:\n",
                "            print(\"[Single Process] DDP variables not found. Running in standalone mode.\")\n",
                "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "            return device, 0, 0, 1\n",
                "\n",
                "    def cleanup_ddp():\n",
                "        if dist.is_initialized():\n",
                "            dist.destroy_process_group()\n",
                "\n",
                "    # --- 3. Model Architecture ---\n",
                "    class AudioTransformer(nn.Module):\n",
                "        def __init__(self, num_classes=35, d_model=128, nhead=4, num_layers=2):\n",
                "            super().__init__()\n",
                "            self.encoder = nn.TransformerEncoder(\n",
                "                nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True),\n",
                "                num_layers=num_layers\n",
                "            )\n",
                "            self.classifier = nn.Linear(d_model, num_classes)\n",
                "\n",
                "        def forward(self, x):\n",
                "            x = x.permute(0, 2, 1)\n",
                "            x = self.encoder(x)\n",
                "            x = x.mean(dim=1)\n",
                "            return self.classifier(x)\n",
                "\n",
                "    # --- 4. Dataset Wrapper ---\n",
                "    class SpeechDataset(Dataset):\n",
                "        def __init__(self, audio_paths, label_map):\n",
                "            self.audio_paths = audio_paths\n",
                "            self.label_map = label_map\n",
                "            self.transform = torchaudio.transforms.MelSpectrogram(n_mels=128)\n",
                "\n",
                "        def __len__(self): \n",
                "            return len(self.audio_paths)\n",
                "\n",
                "        def __getitem__(self, idx):\n",
                "            try:\n",
                "                path, label = self.audio_paths[idx]\n",
                "\n",
                "                # FIX: Explicitly specify soundfile backend instead of torchcodec\n",
                "                waveform, _ = torchaudio.load(path, backend=\"soundfile\")\n",
                "                \n",
                "                target_len = 16000\n",
                "                if waveform.size(1) < target_len:\n",
                "                    waveform = torch.nn.functional.pad(waveform, (0, target_len - waveform.size(1)))\n",
                "                else:\n",
                "                    waveform = waveform[:, :target_len]\n",
                "                \n",
                "                spec = self.transform(waveform).squeeze(0)\n",
                "                return spec, self.label_map[label]\n",
                "            except Exception as e:\n",
                "                # Log error but don't crash the worker\n",
                "                # print(f\"Error loading audio file: {e}\") \n",
                "                return torch.zeros(128, 81), 0  \n",
                "\n",
                "    def collate_fn(batch):\n",
                "        specs, labels = zip(*batch)\n",
                "        return torch.stack(specs), torch.tensor(labels, dtype=torch.long)\n",
                "\n",
                "    # --- 5. Main Training Logic ---\n",
                "    try:\n",
                "        device, local_rank, rank, world_size = setup_ddp()\n",
                "\n",
                "        if rank == 0:\n",
                "            print(\"Verifying/Downloading dataset...\")\n",
                "            torchaudio.datasets.SPEECHCOMMANDS(root=str(data_path), download=True)\n",
                "        \n",
                "        if dist.is_initialized():\n",
                "            dist.barrier() \n",
                "\n",
                "        data_root = data_path / \"SpeechCommands\" / \"speech_commands_v0.02\"\n",
                "        labels = sorted([d.name for d in data_root.iterdir() if d.is_dir() and not d.name.startswith(\"_\")])\n",
                "        label_map = {l: i for i, l in enumerate(labels)}\n",
                "        \n",
                "        all_files = []\n",
                "        for label in labels:\n",
                "            for f in (data_root / label).glob(\"*.wav\"):\n",
                "                all_files.append((str(f), label))\n",
                "        \n",
                "        random.shuffle(all_files)\n",
                "        train_files = all_files[:1000]\n",
                "\n",
                "        dataset = SpeechDataset(train_files, label_map)\n",
                "        sampler = DistributedSampler(dataset) if dist.is_initialized() else None\n",
                "        loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, \n",
                "                          shuffle=(sampler is None), collate_fn=collate_fn, num_workers=0)\n",
                "\n",
                "        model = AudioTransformer(num_classes=len(labels)).to(device)\n",
                "        if dist.is_initialized():\n",
                "            model = DDP(model, device_ids=[local_rank] if torch.cuda.is_available() else None)\n",
                "\n",
                "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
                "        criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "        print(f\"[Rank {rank}] Starting training on {len(labels)} classes...\")\n",
                "        for epoch in range(1, EPOCHS + 1):\n",
                "            model.train()\n",
                "            if sampler: \n",
                "                sampler.set_epoch(epoch)\n",
                "            \n",
                "            total_loss = 0.0\n",
                "            for i, (data, target) in enumerate(loader):\n",
                "                data, target = data.to(device), target.to(device)\n",
                "                optimizer.zero_grad()\n",
                "                output = model(data)\n",
                "                loss = criterion(output, target)\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "                total_loss += loss.item()\n",
                "            \n",
                "            avg_loss = total_loss / len(loader)\n",
                "            if rank == 0:\n",
                "                print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_loss:.4f}\")\n",
                "\n",
                "        if rank == 0:\n",
                "            model_path = output_dir / \"speech_model.pt\"\n",
                "            if dist.is_initialized():\n",
                "                torch.save(model.module.state_dict(), model_path)\n",
                "            else:\n",
                "                torch.save(model.state_dict(), model_path)\n",
                "            print(f\"Model saved to {model_path}\")\n",
                "\n",
                "        cleanup_ddp()\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"[Error] Training failed: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        cleanup_ddp()\n",
                "        raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Training Locally\n",
                "\n",
                "Before scaling to a cluster, we test the code locally using the `TrainerClient`. We use the `torch-distributed` runtime which creates a local DDP environment (simulating a cluster node)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kubeflow.trainer import CustomTrainer, TrainerClient, LocalProcessBackendConfig\n",
                "\n",
                "# Initialize the client with LocalProcessBackend to run locally\n",
                "client = TrainerClient(\n",
                "    backend_config=LocalProcessBackendConfig(cleanup_venv=True)\n",
                ")\n",
                "\n",
                "local_runtime = None\n",
                "for runtime in client.list_runtimes():\n",
                "    if runtime.name == \"torch-distributed\":\n",
                "        local_runtime = runtime\n",
                "        break\n",
                "\n",
                "if not local_runtime:\n",
                "    raise ValueError(\"Local runtime 'torch-distributed' not found\")\n",
                "\n",
                "print(\"Starting local training...\")\n",
                "print(\"Note: First run will download the dataset (~2GB), which may take a few minutes.\\n\")\n",
                "\n",
                "# Start training (removed torchcodec from packages)\n",
                "local_job_name = client.train(\n",
                "    trainer=CustomTrainer(\n",
                "        func=train_speech_recognition,\n",
                "        packages_to_install=[\"torch\", \"torchaudio\", \"librosa\", \"soundfile\", \"tensorboard\"],\n",
                "    ),\n",
                "    runtime=local_runtime, \n",
                ")\n",
                "\n",
                "print(f\"Job {local_job_name} started. Streaming logs...\\n\")\n",
                "try:\n",
                "    for logline in client.get_job_logs(local_job_name, follow=True):\n",
                "        print(logline, end='')\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\n\\nLog streaming interrupted by user.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Distributed Training (Kubernetes)\n",
                "\n",
                "Now we scale the job to a Kubernetes cluster. This requires:\n",
                "1. A Kubernetes cluster with the **Kubeflow Training Operator** installed.\n",
                "2. The **TrainingRuntime** CRD (`torch-distributed`) installed on the cluster.\n",
                "3. Access via `~/.kube/config`.\n",
                "\n",
                "Based on your cluster's resources, we use a minimal configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
                "\n",
                "client = TrainerClient()\n",
                "\n",
                "print(\"Submitting distributed TrainJob...\")\n",
                "\n",
                "try:\n",
                "    k8s_job_name = client.train(\n",
                "        trainer=CustomTrainer(\n",
                "            func=train_speech_recognition,\n",
                "            num_nodes=1,  \n",
                "            resources_per_node={\n",
                "                \"cpu\": \"2\",\n",
                "                \"memory\": \"4Gi\",\n",
                "            },\n",
                "            # Removed torchcodec from packages\n",
                "            packages_to_install=[\"torch\", \"torchaudio\", \"librosa\", \"soundfile\", \"tensorboard\"],\n",
                "        ),\n",
                "        runtime=\"torch-distributed\", \n",
                "    )\n",
                "    print(f\"TrainJob '{k8s_job_name}' submitted successfully!\")\n",
                "    print(f\"\\nYou can check the job status with: kubectl get trainjobs {k8s_job_name}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Failed to submit TrainJob: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Check Status & Logs\n",
                "\n",
                "Verify the job status and view real-time logs from the master replica."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!kubectl get trainjobs\n",
                "!kubectl get pods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'k8s_job_name' in locals():\n",
                "    print(f\"Waiting for job {k8s_job_name} to start running...\")\n",
                "    \n",
                "    try:\n",
                "        # INCREASED TIMEOUT to 1200s (20 minutes)\n",
                "        client.wait_for_job_status(name=k8s_job_name, status={\"Running\"}, timeout=1200)\n",
                "        \n",
                "        print(f\"Job is running! Streaming logs...\\n\")\n",
                "        for logline in client.get_job_logs(k8s_job_name, follow=True):\n",
                "            print(logline, end='')\n",
                "            \n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\n\\nLog streaming interrupted by user.\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\nError while waiting for job or streaming logs: {e}\")\n",
                "        # Manual fallback\n",
                "        print(f\"You can manually check logs with: kubectl logs -l training.kubeflow.org/job-name={k8s_job_name}\")\n",
                "else:\n",
                "    print(\"No Kubernetes job found. Please run the previous cell to create a job first.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
