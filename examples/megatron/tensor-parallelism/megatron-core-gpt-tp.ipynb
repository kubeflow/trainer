{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Distributed Tensor Parallelism with Megatron-Core GPT Model\n",
    "\n",
    "This example demonstrates how to train a GPT model using [Megatron-Core](https://github.com/NVIDIA/Megatron-LM) with [Tensor Parallelism (TP)](https://docs.nvidia.com/nemo/megatron-bridge/0.2.0/parallelisms.html) on Kubeflow Trainer.\n",
    "\n",
    "Tensor Parallelism splits individual model layer weight matrices across multiple GPUs, allowing you to train models that are too large to fit on a single GPU. Since Megatron-Core uses `torchrun` as its distributed launcher, it works natively with the existing `torch-distributed` ClusterTrainingRuntime.\n",
    "\n",
    "This notebook is based on the official [run_simple_mcore_train_loop.py](https://github.com/NVIDIA/Megatron-LM/blob/main/examples/run_simple_mcore_train_loop.py) from the Megatron-LM repository.\n",
    "\n",
    "Megatron-Core Quickstart: https://docs.nvidia.com/megatron-core/developer-guide/latest/user-guide/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**GPU Requirement**: This notebook requires at least **2 NVIDIA GPUs** on a single node. Megatron-Core requires CUDA and uses the NCCL backend for distributed communication.\n",
    "\n",
    "**Training Runtimes**: Make sure the Kubeflow Trainer Controller Manager and Training Runtimes are installed. Follow the [installation guide](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## Define the Training Function\n",
    "\n",
    "The first step is to create function to train GPT model using Megatron-Core with Tensor Parallelism.\n",
    "\n",
    "This training function is based on the official [run_simple_mcore_train_loop.py](https://github.com/NVIDIA/Megatron-LM/blob/main/examples/run_simple_mcore_train_loop.py) from the Megatron-LM repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_megatron_gpt_tp():\n",
    "    import os\n",
    "    import torch\n",
    "    from torch.optim import Adam\n",
    "    from torch.utils.data import DataLoader\n",
    "    from functools import partial\n",
    "    from pathlib import Path\n",
    "\n",
    "    from megatron.core import parallel_state\n",
    "    from megatron.core import dist_checkpointing\n",
    "    from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "    from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "    from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "    from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "    from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "    from megatron.core.datasets.utils import compile_helpers\n",
    "    from megatron.core.datasets.blended_megatron_dataset_builder import (\n",
    "        BlendedMegatronDatasetBuilder,\n",
    "    )\n",
    "    from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "    from megatron.core.distributed import DistributedDataParallel\n",
    "    from megatron.core.distributed import DistributedDataParallelConfig\n",
    "    from megatron.core.distributed.finalize_model_grads import finalize_model_grads\n",
    "    from megatron.core.tokenizers import MegatronTokenizer\n",
    "\n",
    "    _SEQUENCE_LENGTH = 64\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 1: Initialize torch.distributed and Megatron model parallel\n",
    "    # ----------------------------------------------------------------\n",
    "    # tensor_model_parallel_size=2 means each layer's weight matrices\n",
    "    # are split across 2 GPUs. This is the core of Tensor Parallelism.\n",
    "    parallel_state.destroy_model_parallel()\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    torch.distributed.init_process_group(\n",
    "        backend=\"nccl\", rank=rank, world_size=world_size\n",
    "    )\n",
    "\n",
    "    tensor_model_parallel_size = 2\n",
    "    pipeline_model_parallel_size = 1\n",
    "    parallel_state.initialize_model_parallel(\n",
    "        tensor_model_parallel_size, pipeline_model_parallel_size\n",
    "    )\n",
    "\n",
    "    # Set a fixed seed for reproducibility across tensor-parallel ranks.\n",
    "    model_parallel_cuda_manual_seed(123)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 2: Build a small GPT model\n",
    "    # ----------------------------------------------------------------\n",
    "    transformer_config = TransformerConfig(\n",
    "        num_layers=2,\n",
    "        hidden_size=12,\n",
    "        num_attention_heads=4,\n",
    "        use_cpu_initialization=True,\n",
    "        pipeline_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    gpt_model = GPTModel(\n",
    "        config=transformer_config,\n",
    "        transformer_layer_spec=get_gpt_layer_local_spec(),\n",
    "        vocab_size=100,\n",
    "        max_sequence_length=_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpt_model.to(device)\n",
    "\n",
    "    # Wrap with DistributedDataParallel for gradient synchronization.\n",
    "    ddp_config = DistributedDataParallelConfig(\n",
    "        grad_reduce_in_fp32=False,\n",
    "        overlap_grad_reduce=False,\n",
    "        use_distributed_optimizer=False,\n",
    "    )\n",
    "    gpt_model = DistributedDataParallel(\n",
    "        config=transformer_config,\n",
    "        ddp_config=ddp_config,\n",
    "        module=gpt_model,\n",
    "    )\n",
    "\n",
    "    optim = Adam(gpt_model.parameters())\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 3: Prepare a mock dataset (no real data download needed)\n",
    "    # ----------------------------------------------------------------\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            compile_helpers()\n",
    "        torch.distributed.barrier()\n",
    "    else:\n",
    "        compile_helpers()\n",
    "\n",
    "    config = GPTDatasetConfig(\n",
    "        random_seed=0,\n",
    "        sequence_length=_SEQUENCE_LENGTH,\n",
    "        reset_position_ids=False,\n",
    "        reset_attention_mask=False,\n",
    "        eod_mask_loss=False,\n",
    "        tokenizer=MegatronTokenizer.from_pretrained(\n",
    "            metadata_path={\"library\": \"null-text\"},\n",
    "            vocab_size=_SEQUENCE_LENGTH,\n",
    "        ),\n",
    "        mid_level_dataset_surplus=0.005,\n",
    "    )\n",
    "\n",
    "    datasets = BlendedMegatronDatasetBuilder(\n",
    "        MockGPTDataset, [1000, None, None], lambda: True, config\n",
    "    ).build()\n",
    "\n",
    "    train_dataloader = DataLoader(datasets[0], batch_size=8, shuffle=True)\n",
    "    train_iterator = iter(train_dataloader)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 4: Define forward step function\n",
    "    # ----------------------------------------------------------------\n",
    "    def forward_step_func(data_iterator, model):\n",
    "        def loss_func(loss_mask, output_tensor):\n",
    "            losses = output_tensor.float()\n",
    "            loss_mask = loss_mask.view(-1).float()\n",
    "            loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n",
    "            return loss, {\"lm loss\": loss}\n",
    "\n",
    "        data = next(data_iterator)\n",
    "        tokens = data[\"tokens\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        position_ids = data[\"position_ids\"].to(device)\n",
    "        labels = data[\"labels\"].to(device)\n",
    "        loss_mask = data[\"loss_mask\"].to(device)\n",
    "\n",
    "        output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n",
    "\n",
    "        return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 5: Training loop â€” 5 iterations\n",
    "    # ----------------------------------------------------------------\n",
    "    forward_backward_func = get_forward_backward_func()\n",
    "\n",
    "    for iteration in range(5):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        losses_reduced = forward_backward_func(\n",
    "            forward_step_func=forward_step_func,\n",
    "            data_iterator=train_iterator,\n",
    "            model=gpt_model,\n",
    "            num_microbatches=1,\n",
    "            seq_length=_SEQUENCE_LENGTH,\n",
    "            micro_batch_size=8,\n",
    "            decoder_seq_length=_SEQUENCE_LENGTH,\n",
    "            forward_only=False,\n",
    "        )\n",
    "\n",
    "        # Synchronize gradients across TP and DP groups.\n",
    "        finalize_model_grads([gpt_model])\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        print(f\"Iteration {iteration}: Losses reduced: {losses_reduced}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 6: Save and load a distributed checkpoint\n",
    "    # ----------------------------------------------------------------\n",
    "    ckpt_path = os.getcwd() + \"/ckpt\"\n",
    "    Path(ckpt_path).mkdir(exist_ok=True)\n",
    "\n",
    "    model_to_save = gpt_model.module if hasattr(gpt_model, \"module\") else gpt_model\n",
    "    sharded_state_dict = model_to_save.sharded_state_dict(prefix=\"\")\n",
    "    dist_checkpointing.save(\n",
    "        sharded_state_dict=sharded_state_dict, checkpoint_dir=ckpt_path\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "    sharded_state_dict = model_to_save.sharded_state_dict(prefix=\"\")\n",
    "    checkpoint = dist_checkpointing.load(\n",
    "        sharded_state_dict=sharded_state_dict, checkpoint_dir=ckpt_path\n",
    "    )\n",
    "    model_to_save.load_state_dict(checkpoint)\n",
    "    print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "    torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## List the Training Runtimes\n",
    "\n",
    "You can get the list of available Training Runtimes to start your TrainJob.\n",
    "\n",
    "We use the `torch-distributed` runtime since Megatron-Core uses `torchrun` natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
    "\n",
    "client = TrainerClient()\n",
    "\n",
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed\":\n",
    "        torch_runtime = runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Run the Distributed TrainJob\n",
    "\n",
    "Kubeflow TrainJob will train the above GPT model on 1 node with 2 GPUs using Tensor Parallelism (`tensor_model_parallel_size=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_megatron_gpt_tp,\n",
    "        num_nodes=1,\n",
    "        resources_per_node={\n",
    "            \"memory\": \"16Gi\",\n",
    "            \"nvidia.com/gpu\": 2,\n",
    "        },\n",
    "        packages_to_install=[\"megatron-core\"],\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## Check the TrainJob steps\n",
    "\n",
    "You can check the components of TrainJob that's created.\n",
    "\n",
    "You can get the individual status for each of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the running status.\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Watch the TrainJob logs\n",
    "\n",
    "We can use the `get_job_logs()` API to get the TrainJob logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for logline in client.get_job_logs(job_name, follow=True):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## Delete the TrainJob\n",
    "\n",
    "When the TrainJob is finished, you can delete the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
