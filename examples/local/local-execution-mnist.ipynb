{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Kubeflow Trainer: Local Training with Multiple Backends\n",
    "\n",
    "This notebook demonstrates how to run Kubeflow Trainer locally using **three different backends** and shows how easy it is to switch between them.\n",
    "\n",
    "## Available Backends\n",
    "\n",
    "| Backend | Container Runtime | Distributed Training | Best For |\n",
    "|---------|------------------|---------------------|----------|\n",
    "| **Local Process** | None (native Python) | ❌ Single process only | Quick testing, debugging |\n",
    "| **Docker** | Docker required | ✅ Multi-node (PyTorch DDP) | Standard container workflow |\n",
    "| **Podman** | Podman required | ✅ Multi-node (PyTorch DDP) | Rootless containers, security |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **All backends**: Python 3.8+\n",
    "- **Local Process**: No additional requirements\n",
    "- **Docker**: Docker Desktop or Docker Engine\n",
    "- **Podman**: Podman installed\n",
    "\n",
    "This example trains a CNN on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## Install Kubeflow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base installation (required)\n",
    "%pip install -U kubeflow-trainer\n",
    "\n",
    "# Optional extras for container backends:\n",
    "# %pip install -U kubeflow[docker]  # For Docker\n",
    "# %pip install -U kubeflow[podman]  # For Podman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-functions-header",
   "metadata": {},
   "source": [
    "## Define Training Functions\n",
    "\n",
    "We define two training functions:\n",
    "\n",
    "1. **Single-process** - For Local Process Backend (no distributed training)\n",
    "2. **Distributed** - For Docker/Podman Backends (PyTorch DDP across multiple nodes)\n",
    "\n",
    "### Single-Process Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_process():\n",
    "    \"\"\"Train Fashion MNIST CNN - single process (no distributed).\"\"\"\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn, optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    print(\" Starting single-process training...\")\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "    model = Net()\n",
    "    dataset = datasets.FashionMNIST(\n",
    "        './data', train=True, download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    )\n",
    "    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "    for epoch in range(1, 3):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.nll_loss(model(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), \"fashion_mnist_cnn.pt\")\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-dist-header",
   "metadata": {},
   "source": [
    "### Distributed Training Function (PyTorch DDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-distributed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed():\n",
    "    \"\"\"Train Fashion MNIST CNN - distributed across multiple nodes with PyTorch DDP.\"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "    # Setup distributed training\n",
    "    device, backend = (\"cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n",
    "    dist.init_process_group(backend=backend)\n",
    "    rank = dist.get_rank()\n",
    "    \n",
    "    print(f\"Node {rank}/{dist.get_world_size()} - Device: {device}, Backend: {backend}\")\n",
    "\n",
    "    # Create model with DDP\n",
    "    device = torch.device(f\"{device}:{local_rank}\")\n",
    "    model = nn.parallel.DistributedDataParallel(Net().to(device))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # Load data with distributed sampler\n",
    "    data_dir = f\"/tmp/fashion-mnist-{rank}\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    dataset = datasets.FashionMNIST(\n",
    "        data_dir, train=True, download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    )\n",
    "    train_loader = DataLoader(dataset, batch_size=100, sampler=DistributedSampler(dataset))\n",
    "\n",
    "    # Training loop\n",
    "    dist.barrier()\n",
    "    for epoch in range(1, 3):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.nll_loss(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0 and rank == 0:\n",
    "                print(f\"Epoch {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} \"\n",
    "                      f\"({100.0 * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(\"Distributed training complete!\")\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backend-selection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backend Selection\n",
    "\n",
    "Choose which backend to use by running **ONE** of the following cells:\n",
    "\n",
    "### Option 1: Local Process Backend\n",
    "\n",
    "**Use when**: Quick testing, debugging, no containers needed  \n",
    "**Note**: Only supports single-process training (no distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backend-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient, LocalProcessBackendConfig\n",
    "\n",
    "# Configure Local Process Backend\n",
    "backend_config = LocalProcessBackendConfig(\n",
    "    cleanup_venv=True  # Auto-cleanup virtual environments\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "backend = \"Local Process\"\n",
    "training_function = train_single_process  # Single process only\n",
    "num_nodes = None  # Not applicable for local process\n",
    "packages = [\"torch\", \"torchvision\"]\n",
    "\n",
    "print(f\" Selected: {backend} Backend\")\n",
    "print(f\"   Training mode: Single process (no distributed)\")\n",
    "print(f\"   Container runtime: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backend-docker-header",
   "metadata": {},
   "source": [
    "### Option 2: Docker Backend\n",
    "\n",
    "**Use when**: Multi-node distributed training, standard container workflow  \n",
    "**Requires**: Docker Desktop or Docker Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backend-docker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient, ContainerBackendConfig\n",
    "import os\n",
    "# Configure Docker Backend\n",
    "backend_config = ContainerBackendConfig()\n",
    "# backend_config = ContainerBackendConfig(runtime=\"docker\")  # Force Docker if both Docker/Podman installed\n",
    "\n",
    "# Optional: For Colima on macOS\n",
    "# backend_config = ContainerBackendConfig(\n",
    "#     container_host=f\"unix://{os.path.expanduser('~')}/.colima/default/docker.sock\"\n",
    "# )\n",
    "\n",
    "# Set training parameters\n",
    "backend = \"Docker\"\n",
    "training_function = train_distributed  # Distributed training with PyTorch DDP\n",
    "num_nodes = 3  # Number of Docker containers (training nodes)\n",
    "packages = [\"torchvision\"]\n",
    "\n",
    "print(f\"   Selected: {backend} Backend\")\n",
    "print(f\"   Training mode: Distributed (PyTorch DDP)\")\n",
    "print(f\"   Number of nodes: {num_nodes}\")\n",
    "print(f\"   Container runtime: Docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backend-podman-header",
   "metadata": {},
   "source": [
    "### Option 3: Podman Backend\n",
    "\n",
    "**Use when**: Rootless containers, security-focused environments  \n",
    "**Requires**: Podman installed\n",
    "\n",
    "**Docker vs Podman**:\n",
    "- Podman: Daemonless, rootless containers (better security)\n",
    "- Docker: Daemon-based, typically requires root privileges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backend-podman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient, ContainerBackendConfig\n",
    "\n",
    "# Configure Podman Backend\n",
    "backend_config = ContainerBackendConfig(runtime=\"podman\")  # Specify Podman\n",
    "\n",
    "# Optional: Custom Podman socket\n",
    "# backend_config = ContainerBackendConfig(\n",
    "#     runtime=\"podman\",\n",
    "#     container_host=\"unix:///run/user/1000/podman/podman.sock\"\n",
    "# )\n",
    "\n",
    "# Set training parameters\n",
    "backend = \"Podman\"\n",
    "training_function = train_distributed  # Distributed training with PyTorch DDP\n",
    "num_nodes = 3  # Number of Podman containers (training nodes)\n",
    "packages = [\"torchvision\"]\n",
    "\n",
    "print(f\"   Selected: {backend} Backend\")\n",
    "print(f\"   Training mode: Distributed (PyTorch DDP)\")\n",
    "print(f\"   Number of nodes: {num_nodes}\")\n",
    "print(f\"   Container runtime: Podman (rootless)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ovc55bknz3",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced: Custom Runtime Sources (Optional)\n",
    "\n",
    "By default, container backends (Docker/Podman) use training runtimes from:\n",
    "1. **GitHub** - `github://kubeflow/trainer` (official runtimes)\n",
    "2. **Fallback** - Built-in default images (e.g., PyTorch official image)\n",
    "\n",
    "You can customize where runtimes are loaded from by configuring `runtime_source`. This is useful for:\n",
    "- Using custom container images\n",
    "- Loading runtimes from private repositories\n",
    "- Testing local runtime definitions\n",
    "\n",
    "**To use custom sources**, run the cell below after selecting your backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gi5sky5katm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify to use custom runtime sources\n",
    "# from kubeflow.trainer import TrainingRuntimeSource\n",
    "\n",
    "# backend_config.runtime_source = TrainingRuntimeSource(sources=[\n",
    "#     \"github://kubeflow/trainer\",                    # Official Kubeflow runtimes (default)\n",
    "#     \"github://myorg/myrepo/path/to/runtimes\",       # Custom GitHub repository\n",
    "#     \"https://example.com/custom-runtime.yaml\",      # HTTP(S) endpoint\n",
    "#     \"file:///absolute/path/to/runtime.yaml\",        # Local YAML file\n",
    "#     \"/absolute/path/to/runtime.yaml\",               # Local YAML file (alternate syntax)\n",
    "# ])\n",
    "\n",
    "# Sources are checked in priority order. If no runtime is found in any source,\n",
    "# the system falls back to the default image for the framework (e.g., pytorch/pytorch)\n",
    "\n",
    "print(\"Using default runtime sources (no customization needed for this example)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-client",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initialize Client\n",
    "\n",
    "Initialize the TrainerClient with your selected backend configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TrainerClient(backend_config=backend_config)\n",
    "\n",
    "print(f\"\\n TrainerClient initialized with {backend} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list-runtimes",
   "metadata": {},
   "source": [
    "## List Available Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-runtimes",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available training runtimes:\\n\")\n",
    "for runtime in client.list_runtimes():\n",
    "    print(f\"  • {runtime}\")\n",
    "    if runtime.name == \"torch-distributed\":\n",
    "        torch_runtime = runtime\n",
    "\n",
    "print(f\"\\n Selected runtime: {torch_runtime.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-training",
   "metadata": {},
   "source": [
    "## Start Training Job\n",
    "\n",
    "Launch the training job with your selected backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer\n",
    "\n",
    "# Build trainer config based on selected backend\n",
    "trainer_config = {\n",
    "    \"func\": training_function,\n",
    "    \"packages_to_install\": packages,\n",
    "}\n",
    "\n",
    "# Add num_nodes only for distributed backends\n",
    "if num_nodes is not None:\n",
    "    trainer_config[\"num_nodes\"] = num_nodes\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(**trainer_config),\n",
    "    runtime=torch_runtime,\n",
    ")\n",
    "\n",
    "print(f\"\\n Training job started: {job_name}\")\n",
    "print(f\"   Backend: {backend}\")\n",
    "if num_nodes:\n",
    "    print(f\"   Nodes: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-status",
   "metadata": {},
   "source": [
    "## Check Job Status\n",
    "\n",
    "View the training job status and steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "job-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.get_job(job_name)\n",
    "\n",
    "print(f\"\\n Job Status:\")\n",
    "print(f\"   Name: {job.name}\")\n",
    "print(f\"   Status: {job.status}\")\n",
    "print(f\"   Created: {job.creation_timestamp}\")\n",
    "print(f\"\\n   Steps:\")\n",
    "for step in job.steps:\n",
    "    devices_info = f\", Devices: {step.device} x {step.device_count}\" if hasattr(step, 'device') else \"\"\n",
    "    print(f\"     • {step.name}: {step.status}{devices_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stream-logs",
   "metadata": {},
   "source": [
    "## Stream Training Logs\n",
    "\n",
    "Watch the training progress in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Streaming logs from {backend} backend (Ctrl+C to stop):\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    for log_line in client.get_job_logs(job_name, follow=True):\n",
    "        print(log_line, end='')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⏸  Log streaming stopped by user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-completion",
   "metadata": {},
   "source": [
    "## Wait for Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    completed_job = client.wait_for_job_status(\n",
    "        name=job_name,\n",
    "        status={\"Complete\"},\n",
    "        timeout=600,\n",
    "        polling_interval=5\n",
    "    )\n",
    "    print(f\"\\n Training job completed successfully!\")\n",
    "except TimeoutError:\n",
    "    print(f\"\\n  Job did not complete within timeout\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-resources",
   "metadata": {},
   "source": [
    "## Optional: Inspect Container Resources\n",
    "\n",
    "For Docker/Podman backends, you can inspect the containers and networks:\n",
    "\n",
    "**Docker:**\n",
    "```bash\n",
    "docker ps --filter label=trainer.kubeflow.ai/trainjob-name\n",
    "docker network ls --filter label=trainer.kubeflow.org/trainjob-name\n",
    "```\n",
    "\n",
    "**Podman:**\n",
    "```bash\n",
    "podman ps --filter label=trainer.kubeflow.ai/trainjob-name\n",
    "podman network ls --filter label=trainer.kubeflow.org/trainjob-name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Delete the training job to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(job_name)\n",
    "print(f\"\\n Job deleted: {job_name}\")\n",
    "print(f\"   Backend resources cleaned up: {backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Switching Backends\n",
    "\n",
    "To switch backends, simply:\n",
    "\n",
    "1. **Run a different backend selection cell** (Option 1, 2, or 3)\n",
    "2. **Re-run all subsequent cells** starting from \"Initialize Client\"\n",
    "\n",
    "### Quick Comparison:\n",
    "\n",
    "```python\n",
    "# Local Process Backend\n",
    "backend_config = LocalProcessBackendConfig(cleanup_venv=True)\n",
    "training_function = train_single_process  # Single process\n",
    "num_nodes = None\n",
    "\n",
    "# Docker Backend\n",
    "backend_config = ContainerBackendConfig()\n",
    "training_function = train_distributed  # Distributed\n",
    "num_nodes = 3\n",
    "\n",
    "# Podman Backend  \n",
    "backend_config = ContainerBackendConfig(runtime=\"podman\")\n",
    "training_function = train_distributed  # Distributed\n",
    "num_nodes = 3\n",
    "```\n",
    "\n",
    "The rest of the workflow remains **exactly the same** regardless of backend!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
